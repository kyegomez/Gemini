{
    "summary": "Gemini implementation focuses on text/image generation using transformer models and multi-modal inputs, employing MultimodalSentencePieceTokenizer and utility classes for embedding various media. It achieves high accuracy through chain-of-thought prompting and factuality training.",
    "details": [
        {
            "comment": "The code describes an open-source Gemini implementation that aims to surpass ChatGPT. It uses a transformer with special decoders for text or image generation, processing multi-modal inputs (texts, audio, images, and videos) through tokenization before conditional decoding generates outputs.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/README.md\":0-10",
            "content": "[![Multi-Modality](agorabanner.png)](https://discord.gg/qUtxnK2NMf)\n# Gemini\n![gemini](gemini.png)\nThe open source implementation of Gemini, the model that will \"eclipse ChatGPT\", it seems to work by directly taking in all modalities all at once into a transformer with special decoders for text or img generation!\n[Join the Agora discord channel to help with the implementation!](https://discord.gg/CMDpRxCV8g) and [Here is the project board:](https://github.com/users/kyegomez/projects/11/views/1)\nThe input sequences for Gemini consist of texts, audio, images, and videos. These inputs are transformed into tokens, which are then processed by a transformer. Subsequently, conditional decoding takes place to generate image outputs. Interestingly, the architecture of Gemini bears resemblance to Fuyu's architecture but is expanded to encompass multiple modalities. Instead of utilizing a visual transformer (vit) encoder, Gemini simply feeds image embeddings directly into the transformer. For Gemini, "
        },
        {
            "comment": "This code is for initializing a Gemini transformer model using the gemini_torch library. The model has smaller dimensions and fewer layers compared to the original design, and it does not use absolute position embeddings.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/README.md\":10-43",
            "content": "the token inputs will likely be indicated by special modality tokens such as [IMG], <img>, [AUDIO], or <audio>. Codi, a component of Gemini, also employs conditional generation and makes use of the tokenized outputs. To implement this model effectively, I intend to initially focus on the image embeddings to ensure their smooth integration. Subsequently, I will proceed with incorporating audio embeddings and then video embeddings.\n# Install\n`pip3 install gemini-torch`\n## Usage\n### Gemini Transformer Usage\n- Base transformer\n- Multi Grouped Query Attn / flash attn\n- rope\n- alibi\n- xpos\n- qk norm\n- no pos embeds\n- kv cache\n```python\nimport torch\nfrom gemini_torch.model import Gemini\n# Initialize model with smaller dimensions\nmodel = Gemini(\n    num_tokens=50432,\n    max_seq_len=4096,  # Reduced from 8192\n    dim=1280,  # Reduced from 2560\n    depth=16,  # Reduced from 32\n    dim_head=64,  # Reduced from 128\n    heads=12,  # Reduced from 24\n    use_abs_pos_emb=False,\n    attn_flash=True,\n    attn_kv_heads=2,\n    qk_norm=True,"
        },
        {
            "comment": "The code above initializes a Gemini model with specific parameters, and then applies it to text, image, and audio inputs. The image input goes through a specially crafted module that patches and reshapes the tensor to match the shape of the text tensors for alignment.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/README.md\":44-86",
            "content": "    attn_qk_norm=True,\n    attn_qk_norm_dim_scale=True,\n)\n# Text shape: [batch, seq_len, dim]\ntext = torch.randint(0, 50432, (1, 4096))  # Reduced seq_len from 8192\n# Img shape: [batch, channels, height, width]\nimg = torch.randn(1, 3, 128, 128)  # Reduced height and width from 256\n# Audio shape: [batch, audio_seq_len, dim]\naudio = torch.randn(1, 64)  # Reduced audio_seq_len from 128\n# Apply model to text and img\ny = model(text, img, audio)\n# Output shape: [batch, seq_len, dim]\nprint(y)\n```\n--------\n### Multi-Modal with Imgs + Audio\n- Img processing through a specially crafted module that takes in img -> patches it -> then reshapes to the shape of the text tensors, [B, seqlen, dim] -> align with text tokens\n```python\nimport torch\nfrom gemini_torch.model import Gemini\n# Initialize model\nmodel = Gemini(\n    num_tokens=50432,\n    max_seq_len=8192,\n    dim=2560,\n    depth=32,\n    dim_head=128,\n    heads=24,\n    use_abs_pos_emb=False,\n    alibi_pos_bias=True,\n    alibi_num_heads=12,\n    rotary_xpos=True,\n    attn_flash=True,"
        },
        {
            "comment": "In this code snippet, a MultimodalSentencePieceTokenizer is used for tokenization. It's set up with the same configuration as LLAMA's tokenizer and includes special tokens for multi-modality input. The code demonstrates how to create an instance of the tokenizer and perform encoding/decoding for audio modality.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/README.md\":87-128",
            "content": "    attn_kv_heads=2,\n    qk_norm=True,\n    attn_qk_norm=True,\n    attn_qk_norm_dim_scale=True,\n)\n# Text shape: [batch, seq_len, dim]\ntext = torch.randint(0, 50432, (1, 8192))\n# Img shape: [batch, channels, height, width]\nimg = torch.randn(1, 3, 256, 256)\n# Audio shape: [batch, audio_seq_len, dim]\naudio = torch.randn(1, 128)\n# Apply model to text and img\ny = model(text, img, audio)\n# Output shape: [batch, seq_len, dim]\nprint(y.shape)\n```\n------\n## Tokenizer\n- Sentencepiece, tokenizer\n- We're using the same tokenizer as LLAMA with special tokens denoting the beginning and end of the multi modality tokens.\n- Does not fully process img, audio, or videos now we need help on that\n```python\nfrom gemini_torch.tokenizer import MultimodalSentencePieceTokenizer\n# Example usage\ntokenizer_name = \"hf-internal-testing/llama-tokenizer\"\ntokenizer = MultimodalSentencePieceTokenizer(tokenizer_name=tokenizer_name)\n# Encoding and decoding examples\nencoded_audio = tokenizer.encode(\"Audio description\", modality=\"audio\")\ndecoded_audio = tokenizer.decode(encoded_audio)"
        },
        {
            "comment": "The code snippet demonstrates two utility classes, `ImageToTextEmbeddings` and `AudioToEmbeddings`, for converting images and audio into embeddings that are compatible with the language transformer model. The `ImageToTextEmbeddings` class takes an image as input, breaks it down into patches, and reshapes the resulting tensor to match the sequence length expected by the transformer model. On the other hand, the `AudioToEmbeddings` class converts audio data into embeddings that are also compatible with the language transformer model. Both classes provide a unified representation for various media types, allowing them to be processed by the same transformer-based model.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/README.md\":130-173",
            "content": "print(\"Encoded audio:\", encoded_audio)\nprint(\"Decoded audio:\", decoded_audio)\n```\n### `ImageToTextEmbeddings`\n- takes in img -> patches -> reshapes to [B, SEQLEN, Dim] to align with transformer\n```python\nimport torch\nfrom gemini_torch.utils import ImageToTextEmbeddings\n# Example usage\nnum_patches = 16\npatch_size = 16\ntransformer_dim = 512\nimg_channels = 3\nseq_len = 50000\nreduced_dim = 256  # Reduced dimension after dimensionality reduction\nmodel = ImageToTextEmbeddings(\n    num_patches, patch_size, transformer_dim, img_channels, seq_len, reduced_dim\n)\n# Dummy image input [BATCH, CHANNELS, HEIGHT, WIDTH]\ndummy_img = torch.randn(1, 3, 64, 64)  # Batch size of 1, 64x64 RGB image\n# Forward pass\nseq_space_output = model(dummy_img)\nprint(seq_space_output.shape)  # Expected shape: [1, 50000, 256]\n```\n### `AudioToEmbeddings`\n- Transforms audio into the same shape as text tensors.\n```python\nimport torch \nfrom gemini_torch.utils import AudioToEmbeddings\n# Example usage\naudio_seq_len = 32000  # Input audio sequence length\nseqlen = 512  # Sequence length to align with the language transformer"
        },
        {
            "comment": "This code initializes an AudioToEmbeddings model with a specified audio sequence length, sequence length, and embedding dimension. It then generates an example input tensor and passes it through the model to obtain output embeddings. The shape of the output is printed for verification purposes.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/README.md\":174-202",
            "content": "dim = 512  # Embedding dimension\nmodel = AudioToEmbeddings(audio_seq_len, seqlen, dim)\naudio_input = torch.randn(1, audio_seq_len)  # Example input tensor\noutput = model(audio_input)\nprint(\"Output shape:\", output.shape)  # Should be [1, 512, 512]\n```\n# References\n* Combine Reinforcment learning with modular pretrained transformer, multi-modal capabilities, image, audio, \n* self improving mechanisms like robocat\n* PPO? or MPO\n* get good at backtracking and exploring alternative paths\n* speculative decoding\n* Algorithm of Thoughts\n* RLHF\n* [Gemini Report](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf)\n* [Gemini Landing Page](https://deepmind.google/technologies/gemini/#introduction)\n# Todo\n- [ ] [Check out the project board for more todos](https://github.com/users/kyegomez/projects/11/views/1)\n- [ ] Implement the img feature embedder and align imgs with text and pass into transformer: ```Gemini models are trained to accommodate textual input interleaved with a wide variety of au"
        },
        {
            "comment": "The code describes how Gemini models can process various inputs like images, charts, screenshots, PDFs, and videos to produce text or image outputs. The visual encoding of these models is inspired by previous works such as Flamingo, CoCa, and PaLI, but they are multimodal from the beginning, allowing native image output. Additionally, Gemini can ingest audio signals at 16kHz using USM by Google to capture nuances lost in naive text input mapping. Video understanding is achieved by encoding videos as a sequence of frames within the large context window, allowing for seamless integration with other inputs like text or audio.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/README.md\":202-216",
            "content": "dio and visual inputs, such as natural images, charts, screenshots, PDFs, and videos, and they can produce\ntext and image outputs (see Figure 2). The visual encoding of Gemini models is inspired by our own\nfoundational work on Flamingo (Alayrac et al., 2022), CoCa (Yu et al., 2022a), and PaLI (Chen et al.,\n2022), with the important distinction that the models are multimodal from the beginning and can\nnatively output images using discrete image tokens (Ramesh et al., 2021; Yu et al., 2022b).```\n- [ ] Implement the audio processing using USM by Google:```In addition, Gemini can directly ingest audio signals at\n16kHz from Universal Speech Model (USM) (Zhang et al., 2023) features. This enables the model to\ncapture nuances that are typically lost when the audio is naively mapped to a text input (for example,\nsee audio understanding demo on the website).```\n- [ ] Video Processing Technique: \"\nVideo understanding is accomplished by encoding the video as a sequence of frames in the large\ncontext window. Video frames or images can be interleaved naturally with text or audio as part of the"
        },
        {
            "comment": "This code snippet describes the use of a chain-of-thought prompting technique with Gemini Ultra, which achieves highest accuracy. It mentions using k samples (e.g., 8 or 32), a preset threshold for consensus, and maximum likelihood choice when no consensus is reached. The code also discusses training a 1.8B + 3.25 model with Nano-1 and Nano-2 model sizes, focusing on factuality and reasoning performance.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/README.md\":217-231",
            "content": "model input\"\n- [ ] Prompting Technique: ``` We find Gemini Ultra achieves highest\naccuracy when used in combination with a chain-of-thought prompting approach (Wei et al., 2022)\nthat accounts for model uncertainty. The model produces a chain of thought with k samples, for\nexample 8 or 32. If there is a consensus above a preset threshold (selected based on the validation\nsplit), it selects this answer, otherwise it reverts to a greedy sample based on maximum likelihood\nchoice without chain of thought. We refer the reader to appendix for a detailed breakdown of how\nthis approach compares with only chain-of-thought prompting or only greedy sampling.```\n- [ ] Train a 1.8B + 3.25 Model: ```Nano-1 and Nano-2 model sizes are only 1.8B and 3.25B\nparameters respectively. Despite their size, they show exceptionally strong performance on factuality,\ni.e. retrieval-related tasks, and significant performance on reasoning, STEM, coding, multimodal and```"
        }
    ]
}